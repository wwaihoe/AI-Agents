{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (4000) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Load the model on GPU\n",
    "llm = Llama(\n",
    "  model_path=\"../qwen2.5-7b-instruct-q4_k_m.gguf\",\n",
    "  n_gpu_layers=9999,\n",
    "  seed=23,\n",
    "  n_ctx=4000,\n",
    "  verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "class AgentTool:\n",
    "  def __init__(self, llm, name, description, input_schema, output_schema):\n",
    "    self.llm = llm\n",
    "    self.name = name\n",
    "    self.description = description\n",
    "    self.input_schema = input_schema\n",
    "    self.output_schema = output_schema\n",
    "  def __call__(self):\n",
    "    return None\n",
    "  def get_tool_input(self, text_input):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotInput(BaseModel):\n",
    "  text: str\n",
    "\n",
    "class ChatbotOutput(BaseModel):\n",
    "  response: str\n",
    "\n",
    "class Chatbot(AgentTool):\n",
    "  def __init__(self):\n",
    "    super().__init__(\n",
    "      llm=llm,\n",
    "      name=\"Chatbot\",\n",
    "      description=\"An intelligent AI chatbot that can generate human-like responses to text input and can be used to provide information, answer questions, and engage in conversation.\",\n",
    "      input_schema=ChatbotInput,\n",
    "      output_schema=ChatbotOutput\n",
    "    )\n",
    "  def __call__(self, text):\n",
    "    chatbot_input = self.get_tool_input(text)\n",
    "    text = chatbot_input.text\n",
    "    response = self.llm.create_chat_completion(\n",
    "      messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": text\n",
    "        }\n",
    "      ],\n",
    "      max_tokens=1000,\n",
    "    )\n",
    "    response = response['choices'][0]['message']['content']\n",
    "    return ChatbotOutput(response=response)\n",
    "\n",
    "  def get_tool_input(self, text_input):\n",
    "    return ChatbotInput(text=text_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response=\"Hello! I'm just a computer program, so I don't have feelings or consciousness. But I'm here and ready to help you with any questions or conversations you'd like to have. How can I assist you today?\"\n"
     ]
    }
   ],
   "source": [
    "# Test the chatbot\n",
    "chatbot = Chatbot()\n",
    "response = chatbot(\"Hello, how are you?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalculatorInput(BaseModel):\n",
    "  operation: str\n",
    "  a: float\n",
    "  b: float\n",
    "\n",
    "class CalculatorOutput(BaseModel):\n",
    "  result: Optional[float]\n",
    "\n",
    "class Calculator(AgentTool):\n",
    "  def __init__(self):\n",
    "    super().__init__(\n",
    "      llm=llm,\n",
    "      name=\"Calculator\",\n",
    "      description=\"A simple calculator tool for arithmetic operations.\",\n",
    "      input_schema=CalculatorInput,\n",
    "      output_schema=CalculatorOutput\n",
    "    )\n",
    "    \n",
    "  def __call__(self, input_text):\n",
    "    calculator_input = self.get_tool_input(input_text)\n",
    "    operation = calculator_input.operation\n",
    "    a = calculator_input.a\n",
    "    b = calculator_input.b\n",
    "    result = None\n",
    "    if operation == \"add\":\n",
    "      result = self.add(a, b)\n",
    "    elif operation == \"subtract\":\n",
    "      result = self.subtract(a, b)\n",
    "    elif operation == \"multiply\":\n",
    "      result = self.multiply(a, b)\n",
    "    elif operation == \"divide\":\n",
    "      result = self.divide(a, b)\n",
    "    else:\n",
    "      result = None\n",
    "\n",
    "    return CalculatorOutput(result=result)\n",
    "\n",
    "  def get_tool_input(self, input_text):\n",
    "    response = llm.create_chat_completion(\n",
    "      messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"You are a helpful assistant who excels at extracting the operation and numbers for an arithmetic operation from the user input. \n",
    "         Think step-by-step about what operation is being requested and what numbers are being used.\n",
    "         Provide your step-by-step reasoning and response in the following XML format: <think>{step-by-step reasoning}</think> <response><operation>{'add', 'subtract', 'multiply' or 'divide'}</operation><a>{operand a}</a><b>{operand b}</b></response>\"\"\"},\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": f\"Please extract the operation and numbers from the following text: <text>{input_text}</text>. <response>\"\n",
    "        }\n",
    "      ]\n",
    "    )\n",
    "    response_text = response['choices'][0]['message']['content']\n",
    "    operation = response_text.split(\"<operation>\")[1].split(\"</operation>\")[0]\n",
    "    a = float(response_text.split(\"<a>\")[1].split(\"</a>\")[0])\n",
    "    b = float(response_text.split(\"<b>\")[1].split(\"</b>\")[0])\n",
    "    return self.input_schema(operation=operation, a=a, b=b)\n",
    "    \n",
    "  def add(self, a, b):\n",
    "    return a + b\n",
    "\n",
    "  def subtract(self, a, b):\n",
    "    return a - b\n",
    "\n",
    "  def multiply(self, a, b):\n",
    "    return a * b\n",
    "\n",
    "  def divide(self, a, b):\n",
    "    return a / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CalculatorOutput(result=8.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test calculator tool\n",
    "calculator = Calculator()\n",
    "calculator(\"What is 5 plus 3?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class Agent:\n",
    "  def __init__(self, llm, tools: list):\n",
    "    self.llm = llm\n",
    "    self.tools = tools\n",
    "    self.memory = {}\n",
    "\n",
    "  def __call__(self, input: str):\n",
    "    while True:\n",
    "      # select tool\n",
    "      tool = self.get_tool(input, self.tools, self.memory)\n",
    "      if tool is None:\n",
    "        return \"I am unable to help you.\"\n",
    "      else:\n",
    "        # use tool\n",
    "        response = tool(input)\n",
    "        print(\"\\n\\nTool used: \", tool.name)\n",
    "        print(\"Tool input: \", input)\n",
    "        print(\"Tool response: \", response)\n",
    "        # store input and response in memory\n",
    "        if self.memory.get(tool.name) is None:\n",
    "          self.memory[tool.name] = []\n",
    "        self.memory[tool.name].append({\"input\": input, \"response\": response})\n",
    "        # evaluate response\n",
    "        if self.evaluate_response(input, self.memory):\n",
    "          # generate final response if there is sufficient information\n",
    "          print(\"\\n\\nGenerating final response...\")\n",
    "          return self.generate_final_response(input, self.memory)\n",
    "\n",
    "\n",
    "  def get_tool(self, input: str, tools: list, memory: dict):\n",
    "    # create json string with tools with their names and descriptions\n",
    "    tools_dict = [\n",
    "      {\"name\": tool.name, \"description\": tool.description}\n",
    "      for tool in tools\n",
    "    ]\n",
    "\n",
    "    response = self.llm.create_chat_completion(\n",
    "      messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"You are an intelligent AI agent that can make decisions and use tools to solve problems. \n",
    "        Given the following problem and existing memory from past interactions, please select the most appropriate tool to solve it. \n",
    "        Think step-by-step before making a decision.\n",
    "        Provide your step-by-step reasoning and response in the following XML format: <think>{step-by-step reasoning}</think> <tool>{tool name}</tool>.\n",
    "        The tool name provided should be one of the tools given in exact.\"\"\"},\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": f\"Given the following tools: <tools>{tools}</tools>, existing memory: <memory>{memory}</memory>, and the following problem: <problem>{input}</problem>, which tool would you use to solve the problem?\"\n",
    "        }\n",
    "      ]\n",
    "    )\n",
    "    response = response['choices'][0]['message']['content']\n",
    "    print(\"\\n\\nChoose tool: \", response)\n",
    "\n",
    "    # extract tool name from response\n",
    "    tool_name = re.search(r'<tool>(.*)</tool>', response).group(1)\n",
    "\n",
    "    # get tool object from tool name\n",
    "    for tool in tools:\n",
    "      if tool.name == tool_name:\n",
    "        return tool\n",
    "      \n",
    "    return None\n",
    "  \n",
    "  def evaluate_response(self, input, memory):\n",
    "    response = self.llm.create_chat_completion(\n",
    "      messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"You are an intelligent AI agent that can evaluate whether there is sufficient information to provide a final response based on the input query and the memory of past interactions. \n",
    "         Think step-by-step before providing your response. \n",
    "         Provide your step-by-step reasoning and response, a yes or no only, in the following XML format: <think>{step-by-step reasoning}</think> <response>{yes or no}</response>.\"\"\"},\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": f\"Given the following input: <input>{input}</input>, and the memory of past interactions: <memory>{memory}</memory>, is there sufficient information to provide a final response?\"\n",
    "        }\n",
    "      ]\n",
    "    )\n",
    "    response = response['choices'][0]['message']['content']\n",
    "    print(\"\\n\\nEvaluate response: \", response)\n",
    "    return re.search(r'<response>(.*)</response>', response).group(1).lower() == \"yes\"\n",
    "  \n",
    "  def generate_final_response(self, input, memory):\n",
    "    response = self.llm.create_chat_completion(\n",
    "      messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"You are an intelligent AI agent that can generate a final response based on the input query and the memory of past interactions. \n",
    "         Think step-by-step before providing your response. \n",
    "         Provide your step-by-step reasoning and response in the following XML format: <think>{step-by-step reasoning}</think> <response>{final response}</response>.\"\"\"},\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": f\"Given the following input: <input>{input}</input>, and the memory of past interactions: <memory>{memory}</memory>, think step-by-step and provide a final response: \"\n",
    "        }\n",
    "      ]\n",
    "    )\n",
    "    response = response['choices'][0]['message']['content']\n",
    "\n",
    "    think = re.search(r'<think>(.*)</think>', response).group(1)\n",
    "    response = re.search(r'<response>(.*)</response>', response).group(1)\n",
    "\n",
    "    print(\"Step-by-step reasoning: \", think)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot\n",
      "An intelligent AI chatbot that can generate human-like responses to text input and can be used to provide information, answer questions, and engage in conversation.\n"
     ]
    }
   ],
   "source": [
    "tools = [chatbot, calculator]\n",
    "print(tools[0].name)\n",
    "print(tools[0].description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Choose tool:  <think>To solve the problem \"What is 3x3?\", I need to determine which tool can perform multiplication. The existing memory does not provide any relevant information, so I will focus on the available tools. The Chatbot does not have the capability to perform calculations, so it is not suitable for this task. The Calculator, on the other hand, is designed to perform arithmetic operations, including multiplication. Therefore, the Calculator is the appropriate tool to use for this problem.</think>\n",
      "<tool>Calculator</tool>\n",
      "\n",
      "\n",
      "Tool used:  Calculator\n",
      "Tool input:  What is 3x3?\n",
      "Tool response:  result=9.0\n",
      "\n",
      "\n",
      "Evaluate response:  <think>The input query is \"What is 3x3?\" and the memory contains a past interaction where the same query was used and the response was 9.0. This indicates that the system has the necessary information to respond to the query.</think>\n",
      "<response>yes</response>\n",
      "\n",
      "\n",
      "Generating final response...\n",
      "Step-by-step reasoning:  First, I will analyze the input query \"What is 3x3?\" to understand that it is a simple multiplication question. The question is asking for the result of multiplying 3 by 3. Next, I will recall the past interaction memory to find any previous responses to similar queries. In the memory, there is a record of a previous interaction where the input was \"What is 3x3?\" and the response was 9.0. Since this is a straightforward calculation, I can confirm that the result is indeed 9.0.\n",
      "\n",
      "\n",
      "What is 3x3?:  The result of 3x3 is 9.0.\n"
     ]
    }
   ],
   "source": [
    "# Test agent\n",
    "agent = Agent(llm=llm, tools=tools)\n",
    "\n",
    "print(\"\\n\\nWhat is 3x3?: \", agent(\"What is 3x3?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Choose tool:  <think>To solve the problem \"Are whales fishes?\", we need to consider the nature of the question. This is a factual question about biology, not a mathematical or conversational one. The existing memory contains a calculation, which is unrelated to the question at hand. Therefore, neither the Chatbot nor the Calculator is suitable for answering this biological question. However, since we must choose from the given tools, we should select the most appropriate one based on the type of question. In this case, the most appropriate tool would be the Chatbot, as it can provide a conversational response to a factual question, even though it may not have the correct information stored in its memory.</think>\n",
      "<tool>Chatbot</tool>\n",
      "\n",
      "\n",
      "Tool used:  Chatbot\n",
      "Tool input:  Are whales fishes?\n",
      "Tool response:  response='No, whales are not fish. Whales are mammals, specifically marine mammals. They breathe air, give birth to live young, and nurse their offspring with milk. Fish, on the other hand, are cold-blooded aquatic vertebrates that breathe through gills.'\n",
      "\n",
      "\n",
      "Evaluate response:  <think>The input query is \"Are whales fishes?\" and the memory contains a past interaction where the chatbot explicitly answered this question. The past interaction provides a detailed and accurate response, which can be directly used to answer the current query.</think>\n",
      "<response>yes</response>\n",
      "\n",
      "\n",
      "Generating final response...\n",
      "Step-by-step reasoning:  The input asks whether whales are fish. From the memory of past interactions, there is a clear previous response to this question. The previous chatbot interaction provided a detailed explanation that whales are not fish but rather mammals. This explanation includes key characteristics that distinguish whales from fish, such as breathing air, giving birth to live young, and nursing with milk. Since the question is the same and the previous response was accurate, I can directly use that response without needing to generate a new one.\n",
      "\n",
      "\n",
      "Are whales fishes?:  No, whales are not fish. Whales are mammals, specifically marine mammals. They breathe air, give birth to live young, and nurse their offspring with milk. Fish, on the other hand, are cold-blooded aquatic vertebrates that breathe through gills.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nAre whales fishes?: \", agent(\"Are whales fishes?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG-chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
